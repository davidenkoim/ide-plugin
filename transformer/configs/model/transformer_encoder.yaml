# @package _group_

name_of_run: transformer_encoder
checkpoints_dir: checkpoints/

usage_tokenizer_type: sub_token
target_tokenizer_type: camel_case
max_epochs: 5
max_sequence_length: 41
max_num_usages: 10
max_target_length: 6
embedding_dim: 512
usage_embedding_dim: 512
target_embedding_dim: 512
num_heads: 8
num_encoder_layers: 3
num_usage_layers: 1
num_decoder_layers: 4
dropout: 0.1
max_lr: 1e-4
batch_size: 512
sequence_encoder_type: transformer
sequence_reducer: sum
gpus: 1
