0	0	<unknownCharacter>
1	1	file
1	2	content
1	3	fw
1	4	lines
1	5	line
1	6	l
1	7	j
1	8	e
1	9	token
1	10	lexerRunner
1	11	p
1	12	tokens
1	13	right
1	14	i
1	15	package
1	16	slp
1	17	.
1	18	core
1	19	io
1	20	;
1	21	import
1	22	java
1	23	BufferedWriter
1	24	File
1	25	FileOutputStream
1	26	IOException
1	27	OutputStreamWriter
1	28	nio
1	29	charset
1	30	StandardCharsets
1	31	util
1	32	List
1	33	stream
1	34	Collectors
1	35	Stream
1	36	CLI
1	37	lexing
1	38	runners
1	39	LexerRunner
1	40	Pair
1	41	public
1	42	class
1	43	Writer
1	44	{
1	45	static
1	46	void
1	47	writeContent
1	48	(
1	49	,
1	50	String
1	51	)
1	52	throws
1	53	try
1	54	=
1	55	new
1	56	UTF_8
1	57	append
1	58	}
1	59	writeLines
1	60	<
1	61	>
1	62	for
1	63	:
1	64	'\n'
1	65	T
1	66	writeAny
1	67	map
1	68	::
1	69	forEach
1	70	->
1	71	collect
1	72	toList
1	73	int
1	74	0
1	75	size
1	76	++
1	77	get
1	78	toString
1	79	if
1	80	-
1	81	1
1	82	'\t'
1	83	catch
1	84	Exception
1	85	printStackTrace
1	86	writeTokenized
1	87	replaceAll
1	88	"\n"
1	89	"\\n"
1	90	"\t"
1	91	"\\t"
1	92	Double
1	93	writeEntropies
1	94	logger
1	95	!=
1	96	null
1	97	lexFile
1	98	left
1	99	getAbsolutePath
1	100	format
1	101	"%s"
1	102	+
1	103	char
1	104	31
1	105	"%.6f"
1	106	return
1	107	dec
1	108	br
1	109	e2
1	110	BufferedReader
1	111	FileInputStream
1	112	InputStreamReader
1	113	UncheckedIOException
1	114	channels
1	115	Channels
1	116	FileChannel
1	117	CharsetDecoder
1	118	CodingErrorAction
1	119	ArrayList
1	120	Reader
1	121	readLines
1	122	newDecoder
1	123	onMalformedInput
1	124	IGNORE
1	125	newReader
1	126	open
1	127	toPath
1	128	|
1	129	System
1	130	err
1	131	println
1	132	"Reader.readLines(): Files.lines failed, reading full file using BufferedReader instead"
1	133	while
1	134	readLine
1	135	add
1	136	HELP
1	137	VERBOSE
1	138	LANGUAGE
1	139	ADD_DELIMS
1	140	PER_LINE
1	141	EXTENSION
1	142	VOCABULARY
1	143	CLOSED
1	144	UNK_CUTOFF
1	145	TRAIN
1	146	ORDER
1	147	GIGA
1	148	TEST
1	149	COUNTER
1	150	MODEL
1	151	SELF
1	152	CACHE
1	153	DYNAMIC
1	154	NESTED
1	155	arguments
1	156	mode
1	157	lexer
1	158	vocabulary
1	159	model
1	160	modelRunner
1	161	args
1	162	message
1	163	language
1	164	counter
1	165	order
1	166	modelName
1	167	counterFile
1	168	t
1	169	m
1	170	translate
1	171	inDir
1	172	outDir
1	173	emptyVocab
1	174	vocabFile
1	175	outFile
1	176	fileProbs
1	177	fileCount
1	178	stats
1	179	f
1	180	testDir
1	181	fileMRRs
1	182	trainDir
1	183	arg
1	184	a
1	185	FileWriter
1	186	DoubleSummaryStatistics
1	187	counting
1	188	Counter
1	189	giga
1	190	GigaCounter
1	191	CounterIO
1	192	trie
1	193	AbstractTrie
1	194	example
1	195	JavaRunner
1	196	NLRunner
1	197	Lexer
1	198	code
1	199	JavaLexer
1	200	simple
1	201	CharacterLexer
1	202	PunctuationLexer
1	203	TokenizedLexer
1	204	WhitespaceLexer
1	205	modeling
1	206	Model
1	207	dynamic
1	208	CacheModel
1	209	NestedModel
1	210	mix
1	211	MixModel
1	212	ngram
1	213	ADMModel
1	214	ADModel
1	215	JMModel
1	216	NGramModel
1	217	WBModel
1	218	ModelRunner
1	219	translating
1	220	Vocabulary
1	221	VocabularyRunner
1	222	/**
1	223	*
1	224	 Provides a command line interface to a runnable jar produced from this source code.
1	225	 Using SLP-Core as a library can give access to more powerful usage;
1	226	 see 
1	227	@link
1	228	 and 
1	229	 to get started.
1	230	@author
1	231	Vincent
1	232	Hellendoorn
1	233	*/
1	234	private
1	235	final
1	236	"(-h|--help)"
1	237	"--verbose"
1	238	"(-l|--language)"
1	239	"--delims"
1	240	"(-pl|--per-line)"
1	241	"(-e|--extension)"
1	242	"(-v|--vocabulary)"
1	243	"--closed"
1	244	"(-u|--unk-cutoff)"
1	245	"(-tr|--train)"
1	246	"(-o|--order)"
1	247	"--giga"
1	248	"(-te|--test)"
1	249	"--counter"
1	250	"(-m|--model)"
1	251	"(-s|--self)"
1	252	"(-c|--cache)"
1	253	"(-d|--dynamic)"
1	254	"(-n|--nested)"
1	255	[
1	256	]
1	257	main
1	258	length
1	259	==
1	260	||
1	261	isSet
1	262	out
1	263	"No arguments provided, printing help menu."
1	264	printHelp
1	265	toLowerCase
1	266	getLexer
1	267	getModel
1	268	setupLexerRunner
1	269	setupVocabulary
1	270	setupModelRunner
1	271	setupLogger
1	272	switch
1	273	case
1	274	"lex"
1	275	lex
1	276	break
1	277	"lex-ix"
1	278	true
1	279	"vocabulary"
1	280	buildVocabulary
1	281	"train"
1	282	train
1	283	"test"
1	284	test
1	285	"train-test"
1	286	trainTest
1	287	"predict"
1	288	predict
1	289	"train-predict"
1	290	trainPredict
1	291	default
1	292	exit
1	293	"Command "
1	294	" not recognized; use -h for help."
1	295	teardownLogger
1	296	"\nMain API for command-line usage of SLP-core: can be used to lex corpora, build vocabularies and train/test/predict."
1	297	"\nAll models are n-gram models at present for the Jar, for more powerful usage, use as library in Java code."
1	298	"\nEvery use-case starts with lexing, so make sure those options are set correctly."
1	299	"\nUsage:"
1	300	"\t-h | --help (or no arguments): Print this help menu)"
1	301	"\tlex <in-path> <out-path> [OPTIONS]: lex the in-path (file or directory) to a mirrored structure in out-path."
1	302	"\n\t\tSee lexing options below, for instance to specify extension filters, delimiter options."
1	303	"\tlex-ix <in-path> <out-path> [OPTIONS]: like lex, excepts translates tokens to integers."
1	304	"\n\t\tNote: if not given a vocabulary, will build one first and write it to 'train.vocab' in same dir as out-path"
1	305	"\tvocabulary <in-path> <out-file> [OPTIONS]: lex the in-path and write resulting vocabulary to out-file."
1	306	"\ttrain --train <path> --counter <out-file> [OPTIONS]: lex all files in in-path, train n-gram model and write to out-file."
1	307	"\n\t\tCurrently the Jar supports n-gram models only; config-file support may come in further revisions."
1	308	"\n\t\tNote: if not given a vocabulary, will build one first and write it to 'train.vocab' in same dir as out-file"
1	309	"\ttest --test <path> --counter <counts-file> -v <vocab-file> [OPTIONS]: test on files in in-path using counter from counts-file."
1	310	"\n\t\tNote that the vocabulary parameter is mandatory; a counter is meaningless without a vocabulary."
1	311	"\n\t\tUse -m (below) to set the model. See also: predict, train-test."
1	312	"\ttrain-test [--train <path>] --test <path> [OPTIONS]: train on train-path or self-test on test-path; tests modeling accuracy on test-path without storing a counter"
1	313	"\tpredict --test <path> --counter <counts-file> [OPTIONS]: test predictions on files in in-path using counter from counts-file."
1	314	"\n\t\tUse -m (below) to set the model. See also: test, train-predict"
1	315	"\ttrain-predict --train <path> --test <path> [OPTIONS]: train on train-path or self-test on test-path; tests prediction accuracy on test-path without storing a counter"
1	316	"\nOptions:"
1	317	"  General:"
1	318	"\t-h | --help: Show this screen"
1	319	"\t--verbose <file>: print all output to file"
1	320	"  Lexing:"
1	321	"\t-l | --language: specify language for tokenization. Currently one of (simple, blank, tokens, java)."
1	322	"\n\t\t Use 'simple' (default) for splitting on punctuation (preserved as tokens) and whitespace (ignored);"
1	323	"\n\t\t use 'blank' for just splitting on whitespace and use 'tokens' for pre-tokenized text."
1	324	"\t--delims: explicitly add line delimiters to the start and end of the input. Default: none"
1	325	"\n\t\tWill add to every line if --per-line is set"
1	326	"\t-pl | --per-line: lex and model each line in isolation. Default: false"
1	327	"\t-e | --extension: use the provided extension regex to filter input files. Default: none filtered"
1	328	"  Vocabulary:"
1	329	"\t-v | --vocabulary: specify file to read vocabulary from."
1	330	"\n\t\tIf none given, vocabulary is constructed 'on the fly' while modeling."
1	331	"\t--closed: close vocabulary after reading, treating every further token as unknown."
1	332	"\n\t\tNot generally recommended for source code, but sometimes applicable."
1	333	"\n\t\tHas no effect if vocabulary is built on-line instead of read from file."
1	334	"\t-u | --unk-cutoff: set an unknow token cut-off when building/reading in vocabulary."
1	335	"\n\t\tAll tokens seen >= cut-off times are preserved. Default: 0, preserving all tokens."
1	336	"  Training:"
1	337	"\t-tr | --train: the path to train on"
1	338	"\t-o | --order: specify order for n-gram models. Default: 6"
1	339	"  Testing:"
1	340	"\t-te | --test: the path to test on"
1	341	"\t--counter: the path to read the counter from, if testing with pre-trained model"
1	342	"\t-m | --model: use specified n-gram smoothing model:"
1	343	"\n\t\tjm = Jelinek-Mercer, wb = Witten-Bell, ad(m) = (modified) absolute discounting"
1	344	"\t-s | --self: specify that we are testing on the train data, implying to 'forget' any data prior to testing."
1	345	"\t-c | --cache: add an n-gram cache model"
1	346	"\t-d | --dynamic: dynamically update all models with test data"
1	347	"\t-n | --nested: build a nested model of test data (sets dynamic to false); see paper for more details"
1	348	setSentenceMarkers
1	349	setExtension
1	350	getArg
1	351	isEmpty
1	352	!
1	353	exists
1	354	"Vocabulary file not found: "
1	355	cutOff
1	356	Integer
1	357	parseInt
1	358	"Retrieving vocabulary from file"
1	359	read
1	360	else
1	361	close
1	362	isSelf
1	363	setSelfTesting
1	364	equals
1	365	"simple"
1	366	"java"
1	367	"tokens"
1	368	"blank"
1	369	"char"
1	370	"Using lexer "
1	371	getClass
1	372	getSimpleName
1	373	wrapModel
1	374	getNGramModel
1	375	getCounter
1	376	?
1	377	DEFAULT_NGRAM_ORDER
1	378	"jm"
1	379	"wb"
1	380	"ad"
1	381	"adm"
1	382	setStandard
1	383	instanceof
1	384	COUNT_OF_COUNTS_CUTOFF
1	385	&&
1	386	getCounterFile
1	387	"No (valid) counter file given for test/predict mode! Specify one with --counter *path-to-counter*"
1	388	long
1	389	currentTimeMillis
1	390	"Retrieving counter from file"
1	391	readCounter
1	392	"Counter retrieved in "
1	393	/
1	394	1000
1	395	"s"
1	396	"Nested mode set, but no test directory given!"
1	397	standard
1	398	setDynamic
1	399	false
1	400	boolean
1	401	>=
1	402	3
1	403	2
1	404	<=
1	405	lexDirectory
1	406	lexDirectoryToIndices
1	407	getVocabularyFile
1	408	getParentFile
1	409	"train.vocab"
1	410	"Writing vocabulary to: "
1	411	write
1	412	"Not enough arguments given."
1	413	"Lexing requires at least two arguments: source and target path."
1	414	"Source path for building vocabulary does not exist: "
1	415	build
1	416	"Building vocabulary requires at least two arguments: source path and output file."
1	417	getTrainFile
1	418	"Source path for training does not exist: "
1	419	learnDirectory
1	420	getCount
1	421	"Writing counter to file"
1	422	writeCounter
1	423	"Counter written in "
1	424	"Writing vocabulary to file"
1	425	"Vocabulary written"
1	426	getTestFile
1	427	"Test path does not exist: "
1	428	"Counter file to use not found: "
1	429	modelDirectory
1	430	getStats
1	431	peek
1	432	printf
1	433	"Testing complete, modeled %d files with %d tokens yielding average entropy:\t%.4f\n"
1	434	getAverage
1	435	trainModel
1	436	"Testing complete, modeled %d files with %d tokens yielding average MRR:\t%.4f\n"
1	437	"No valid train path given, will assume self-testing on test data"
1	438	"No valid test path given for train-test mode, exiting"
1	439	matches
1	440	""
1	441	startsWith
1	442	"train-"
1	443	PRINT_FREQ
1	444	root
1	445	c
1	446	counts
1	447	w
1	448	ordered
1	449	e1
1	450	unkCount
1	451	entry
1	452	count
1	453	x
1	454	split
1	455	index
1	456	word
1	457	Map
1	458	Entry
1	459	1000000
1	460	 Set counts cut-off so that only events seen >= cutOff are considered. Default: 0, which includes every seen token
1	461	 <br />
1	462	 <em>Note:</em> this has been shown to give a distorted perspective on models of particularly source code,
1	463	 but may be applicable in some circumstances
1	464	@param
1	465	The minimum number of counts of an event in order for it to be considered.
1	466	"VocabularyBuilder.cutOff(): negative cut-off given, set to 0 (which includes every token)"
1	467	 Build vocabulary on all files reachable from (constructor) provided root, 
1	468	 possibly filtering by name/extension (see 
1	469	#
1	470	setRegex
1	471	).
1	472	@return
1	473	flatMap
1	474	%
1	475	"Building vocabulary, %dM tokens processed\n"
1	476	Math
1	477	round
1	478	toMap
1	479	sum
1	480	entrySet
1	481	sorted
1	482	compare
1	483	getValue
1	484	getKey
1	485	+=
1	486	store
1	487	UNK
1	488	"Vocabulary constructed on "
1	489	" tokens, size: "
1	490	 Read vocabulary from file, where it is assumed that the vocabulary is written as per 
1	491	 tab-separated, having three columns per line: count, index and token (which may contain tabs))
1	492	 <br /><em>Note:</em>: index is assumed to be strictly incremental starting at 0!
1	493	filter
1	494	forEachOrdered
1	495	"VocabularyRunner.read(): non-consecutive indices while reading vocabulary!"
1	496	 Writes vocabulary to file with one entry per line. Format: tab-separated count, index and word.
1	497	 Note: count is informative only and is not updated during training!
1	498	File to write vocabulary to.
1	499	getCounts
1	500	getWords
1	501	"Error writing vocabulary in Vocabulary.toFile()"
1	502	BOS
1	503	EOS
1	504	wordIndices
1	505	words
1	506	closed
1	507	checkPoint
1	508	indices
1	509	HashMap
1	510	 Translation (to integers) is the second step (after Lexing) before any modeling takes place.
1	511	 The this is global (static) and is open by default; it can be initialized through
1	512	 the 
1	513	thisRunner
1	514	 class or simply left open to be filled by the modeling code
1	515	 (as has been shown to be more appropriate for modeling source code).
1	516	 <em>Note:</em> the counts in this class are for informative purposes only:
1	517	 these are not (to be) used by any model nor updated with training.
1	518	"<UNK>"
1	519	"<s>"
1	520	"</s>"
1	521	this
1	522	addUnk
1	523	put
1	524	setCheckpoint
1	525	restoreCheckpoint
1	526	--
1	527	remove
1	528	set
1	529	toIndices
1	530	toIndex
1	531	toWords
1	532	toWord
1	533	maxOrder
1	534	result
1	535	start
1	536	end
1	537	gram
1	538	sequencing
1	539	 Work in progress, no guarantees of functionality yet.
1	540	SkipGramSequencer
1	541	sequenceBackward
1	542	firstLoc
1	543	NGramSequencer
1	544	sequenceForward
1	545	min
1	546	subList
1	547	sequenceAround
1	548	max
1	549	 Returns the longest possible sublist that doesn't exceed 
1	550	@linkplain
1	551	getNGramOrder
1	552	 in length
1	553	 and <u>includes</u> the token at index 
1	554	@code
1	555	 index
1	556	sequenceAt
1	557	INV_NEG_LOG_2
1	558	GLOBAL_PREDICTION_CUTOFF
1	559	selfTesting
1	560	cutoff
1	561	LEARN_PRINT_INTERVAL
1	562	learnStats
1	563	lexed
1	564	l2
1	565	MODEL_PRINT_INTERVAL
1	566	modelStats
1	567	ent
1	568	mrr
1	569	lineProbs
1	570	lineLengths
1	571	modeled
1	572	entropies
1	573	prevCount
1	574	preds
1	575	mrrs
1	576	probConfs
1	577	probConf
1	578	prob
1	579	conf
1	580	probability
1	581	ix
1	582	p1
1	583	p2
1	584	perLine
1	585	Files
1	586	Path
1	587	IntStream
1	588	 This class can be used to run 
1	589	-related functions over bodies of code.
1	590	 It provides the lexing and translation steps necessary to allow immediate learning and modeling from directories or files.
1	591	 As such, it wraps the pipeline stages 
1	592	 --> 
1	593	 --> Translate (
1	594	) --> 
1	595	 This class uses a 
1	596	, which differentiates between file and line data and provides a some additional utilities.
1	597	 It also provides easier access to self-testing (in which each line is forgotten before modeling it and re-learned after),
1	598	 which is helpful for count-based models such as 
1	599	s.
1	600	double
1	601	1.0
1	602	log
1	603	6
1	604	10
1	605	protected
1	606	 Convenience function that creates a new 
1	607	 instance for the provided 
1	608	 that is backed by the same 
1	609	The model to provide a 
1	610	 for.
1	611	A
1	612	new 
1	613	 for this 
1	614	 		   with the current 
1	615	's 
1	616	copyForModel
1	617	getLexerRunner
1	618	getVocabulary
1	619	 Enables self testing: if we are testing on data that we also trained on, and our models are able to forget events,
1	620	 we can simulated training on all-but one sequence (the one we are modeling) by temporarily forgetting
1	621	 an event, modeling it and re-learning it afterwards. This maximizes use of context information and can be used
1	622	 to simulate full cross-validation.
1	623	If true, will temporarily "forget" every sequence before modeling it and "re-learn" it afterwards
1	624	getPredictionCutoff
1	625	setPredictionCutoff
1	626	notify
1	627	learnTokens
1	628	"Counting complete: %d tokens processed in %ds\n"
1	629	learnFile
1	630	willLexFile
1	631	learnContent
1	632	lexText
1	633	isPerLine
1	634	logLearningProgress
1	635	learn
1	636	"Counting: %dM tokens processed in %ds\n"
1	637	1e6
1	638	forgetDirectory
1	639	walk
1	640	toFile
1	641	isFile
1	642	forgetFile
1	643	forgetTokens
1	644	forgetContent
1	645	forget
1	646	100000
1	647	0.0
1	648	of
1	649	modelTokens
1	650	modelFile
1	651	modelContent
1	652	modelSequence
1	653	logModelingProgress
1	654	toLines
1	655	toProb
1	656	toEntropy
1	657	skip
1	658	mapToDouble
1	659	doubleValue
1	660	summaryStatistics
1	661	getSum
1	662	"Modeling: %dK tokens processed in %ds, avg. entropy: %.4f\n"
1	663	1e3
1	664	predictTokens
1	665	predictFile
1	666	predictContent
1	667	predictSequence
1	668	logPredictionProgress
1	669	toPredictions
1	670	range
1	671	mapToObj
1	672	indexOf
1	673	toMRR
1	674	"Predicting: %dK tokens processed in %ds, avg. MRR: %.4f\n"
1	675	limit
1	676	K
1	677	getFileStats
1	678	global
1	679	modelRunners
1	680	files
1	681	mixed
1	682	baseRunner
1	683	testRoot
1	684	testBaseModel
1	685	baseModelRunner
1	686	next
1	687	input
1	688	lineage
1	689	pos
1	690	lang
1	691	reflect
1	692	InvocationTargetException
1	693	Arrays
1	694	Collections
1	695	AbstractModel
1	696	extends
1	697	getBaseRunner
1	698	newModel
1	699	newModelRunner
1	700	getDeclaredConstructor
1	701	newInstance
1	702	InstantiationException
1	703	IllegalAccessException
1	704	IllegalArgumentException
1	705	SecurityException
1	706	NoSuchMethodException
1	707	 When notified of a new file, update the nesting accordingly
1	708	@
1	709	Override
1	710	updateNesting
1	711	learnToken
1	712	forgetToken
1	713	modelAtIndex
1	714	modelToken
1	715	predictAtIndex
1	716	predictToken
1	717	getMix
1	718	getTestRoot
1	719	getLineage
1	720	clear
1	721	 Returns all non-trivial directories starting from the root file to the new file.
1	722	 Non-trivial meaning a directory containing more than one regex-matching file or dir itself; 
1	723	 building a separate nested model for such directories is pointless.
1	724	The next file to be modeled
1	725	containing all relevant directories from root file inclusive to 
1	726	 file
1	727	 exclusive
1	728	list
1	729	listFiles
1	730	anyMatch
1	731	isDirectory
1	732	reverse
1	733	DEFAULT_CAPACITY
1	734	capacity
1	735	cache
1	736	cachedRefs
1	737	removed
1	738	hash
1	739	ArrayDeque
1	740	Deque
1	741	5000
1	742	getConstructor
1	743	updateCache
1	744	removeFirst
1	745	hashCode
1	746	addLast
1	747	sequence
1	748	mass
1	749	hits
1	750	sub
1	751	resN
1	752	confidence
1	753	predictions
1	754	prediction
1	755	added
1	756	prev
1	757	clazz
1	758	HashSet
1	759	Set
1	760	MapTrieCounter
1	761	abstract
1	762	countBatch
1	763	unCountBatch
1	764	unCount
1	765	modelWithConfidence
1	766	/=
1	767	pow
1	768	addAll
1	769	getTopSuccessors
1	770	Class
1	771	in
1	772	contextCount
1	773	n1
1	774	n2
1	775	n3
1	776	n4
1	777	Y
1	778	Ds
1	779	Ns
1	780	discount
1	781	MLEDisc
1	782	lambda
1	783	super
1	784	getCountofCount
1	785	4
1	786	isNaN
1	787	0.25
1	788	0.6
1	789	getDistinctCounts
1	790	distinctContext
1	791	N1Plus
1	792	MLE
1	793	DEFAULT_LAMBDA
1	794	0.5
1	795	D
1	796	model1
1	797	model2
1	798	modelL
1	799	modelR
1	800	res1
1	801	res2
1	802	predictL
1	803	predictR
1	804	key
1	805	own
1	806	other
1	807	implements
1	808	InverseMixModel
1	809	getLeft
1	810	getRight
1	811	setLeft
1	812	setRight
1	813	notifyLeft
1	814	notifyRight
1	815	pauseDynamic
1	816	unPauseDynamic
1	817	learnLeft
1	818	learnRight
1	819	forgetLeft
1	820	forgetRight
1	821	modelLeft
1	822	modelRight
1	823	predictLeft
1	824	predictRight
1	825	 Mix a pair of probability/confidence for both the left and right model according to some function.
1	826	The lexed and translated input
1	827	The index of the token to model in the input
1	828	Left model's score
1	829	Right model's score
1	830	mixture of the two scores
1	831	 Default mixing implementation of predictions at 
1	832	 from both models.
1	833	 Acquires a consolidated map of predictions, invoking the other model for any non-overlapping predictions.
1	834	 An alternative, faster implementation could be to simply assume zero confidence
1	835	 for any non-overlapping keys, which would be more compatible with batch mode.
1	836	keySet
1	837	containsKey
1	838	continue
1	839	"["
1	840	", "
1	841	"]"
1	842	lNorm
1	843	rNorm
1	844	BiDirectionalModel
1	845	 Use only for stateless models!
1	846	getForward
1	847	getReverse
1	848	 Assumes the two models report their confidence as asymptotically close to 1.
1	849	 The way to mix probabilities is then to compare the inverses of the confidences
1	850	 and use these to weight the respective models' guesses.
1	851	0.999
1	852	ProportionalMixModel
1	853	wasDynamic
1	854	dynamicDepth
1	855	d
1	856	temp
1	857	 Implementation of 
1	858	 interface that serves as base class for most models.<br />
1	859	 This class extends 
1	860	 by imposing per-token control and maintenance
1	861	@see
1	862	, 
1	863	 Enable/disable dynamic updating of model, as implemented by underlying model.
1	864	True if the model should update dynamically
1	865	getConfidence
1	866	 Default implementation of 
1	867	 which invokes 
1	868	 at each index and takes care of dynamic updating after each token.
1	869	 at each index and takes care of dynamic updating for each token.
1	870	 Interface for models, providing the third step after lexing and translating.
1	871	 Implemented primarily through 
1	872	 <br /><br />
1	873	 The interface allows a model to be notified when modeling a new file and furthermore specifies
1	874	 two types of updating (learning and forgetting) and two types of modeling (entropy and prediction).
1	875	 Each update and model operation comes in two flavors: batch and indexed:
1	876	 <ul>
1	877	 <li> The <b>indexed</b> mode is essential for a model to support, allowing for such tasks 
1	878	 as on-the-fly prediction, updating cache models after seeing each token, etc.
1	879	 <li> The <b>batch</b> mode is added because it can yield quite substantial speed-up for some models and 
1	880	 should thus be invoked as often as possible (e.g. as in 
1	881	 It is implemented with simple iteration over calls to indexed mode by default but overriding this is encouraged.
1	882	 </ul>
1	883	 See also 
1	884	, which overrides 
1	885	 to incorporate dynamic
1	886	 updating to models.
1	887	 come with a default implementation which simply invokes the indexed version for each index.
1	888	interface
1	889	 Notifies model of upcoming test file, allowing it to set up accordingly (e.g. for nested models)
1	890	 <em>Note:</em> most models may simply do nothing, but is tentatively left <code>abstract</code> as a reminder.
1	891	File to model next
1	892	 Notify underlying model to learn provided input. May be ignored if no such ability exists.
1	893	 Default implementation simply invokes 
1	894	 for each position in input.
1	895	Lexed and translated input tokens (use 
1	896	 Vocabulary
1	897	 to translate back if needed)
1	898	 Like 
1	899	 but for the specific token at 
1	900	 Primarily used for dynamic updating. Similar to 
1	901	 batch implementation (
1	902	 should be invoked when possible and can provide speed-up.
1	903	Index of token to assign probability/confidence score too.
1	904	 Notify underlying model to 'forget' the provided input, e.g. prior to self-testing.
1	905	 May be ignored if no such ability exists.
1	906	 Any invoking code should note the risk of the underlying model not implementing this!
1	907	 For instance when self-testing, this may lead to testing on train-data.
1	908	 See 
1	909	NestedModelRunner
1	910	 for a good example, which uses this functionality to train on all-but the test file.
1	911	 It currently uses only 
1	912	s which are capable of un-learning input.
1	913	 Part of new interface design. Currently obsolete, do not use.
1	914	 Model each token in input to a pair of probability/confidence (see 
1	915	 The default implementation simply invokes 
1	916	 for each index;
1	917	 can be overriden in favor of batch processing by underlying class if preferable
1	918	 (but remember to implement dynamic updating or caches won't work).
1	919	Probability
1	920	/Confidence Pair for each token in input
1	921	 Model a single token in 
1	922	 input
1	923	 at index 
1	924	 to a pair of (probability, confidence) &isin; ([0,1], [0,1])
1	925	 The probability must be a valid probability, positive and summing to 1 given the context.
1	926	 implements this with dynamic updating support.
1	927	 Since some models implement faster "batch" processing, 
1	928	 should generally be called if possible.
1	929	/Confidence Pair for token at 
1	930	 in 
1	931	 Give top <code>N</code> predictions for each token in input with probability/confidence scores (see 
1	932	/Confidence-weighted set of predictions for each token in input
1	933	 Give top <code>N</code> predictions for position 
1	934	 in input,
1	935	 with corresponding probability/confidence scores as in 
1	936	 The model should produce suggestions for the token that should appear
1	937	 following the first 
1	938	 tokens in 
1	939	, regardless of what token is presently there
1	940	 if any (e.g. it could be an insertion task).
1	941	 Some example interpretations for different values of 
1	942	 <li> 0: predict the first token, without context.
1	943	 <li> 
1	944	 input.size()
1	945	: predict the next token after 
1	946	 <li> 3: predict the 4th token in 
1	947	, regardless of what token is currently at that index.
1	948	successor
1	949	valid
1	950	ArrayStorage
1	951	checkExactSequence
1	952	checkPartialSequence
1	953	successors
1	954	GROWTH_FACTOR
1	955	initSize
1	956	depth
1	957	padding
1	958	o
1	959	oldLen
1	960	newLen
1	961	value
1	962	arr
1	963	ObjectInput
1	964	ObjectOutput
1	965	ArrayTrieCounter
1	966	Object
1	967	1.5
1	968	fill
1	969	MAX_VALUE
1	970	getSuccessors
1	971	valueOf
1	972	getTopSuccessorsInternal
1	973	makeNext
1	974	getSuccessor
1	975	getSuccIx
1	976	removeSuccessor
1	977	arraycopy
1	978	5
1	979	copyOf
1	980	putSuccessor
1	981	grow
1	982	binarySearch
1	983	readExternal
1	984	ClassNotFoundException
1	985	readInt
1	986	readObject
1	987	writeExternal
1	988	writeInt
1	989	getSuccessorCount
1	990	writeObject
1	991	nCounts
1	992	n
1	993	succ
1	994	nearLast
1	995	distinctCounts
1	996	totalDistinct
1	997	countOfCountsI
1	998	trueSucc
1	999	adj
1	1000	arrayCounter
1	1001	newNext
1	1002	singleton
1	1003	toUpdate
1	1004	currIndex
1	1005	prevIndex
1	1006	updateCurr
1	1007	updatePrev
1	1008	volatile
1	1009	 Return a new AbstractTrie instance of your choosing.
1	1010	 For instance, 
1	1011	 at present returns a map for the root and second level, than a regular Trie,
1	1012	 whereas TrieCounter always uses a Trie.
1	1013	getContextCount
1	1014	-=
1	1015	getSuccessorNode
1	1016	update
1	1017	updateCount
1	1018	emptyList
1	1019	synchronized
1	1020	updateSuccessor
1	1021	addArray
1	1022	updateNCounts
1	1023	updateTrie
1	1024	updateArray
1	1025	promoteArrayToMap
1	1026	updateCoCs
1	1027	updateArrayCount
1	1028	promoteArrayToTrie
1	1029	copyOfRange
1	1030	"Attempting to forget unknown event: "
1	1031	pseudoOrdering
1	1032	MAX_DEPTH_MAP_TRIE
1	1033	classKey
1	1034	countsKey
1	1035	cached
1	1036	i1
1	1037	i2
1	1038	topSuccessors
1	1039	curr
1	1040	base
1	1041	it
1	1042	unimi
1	1043	dsi
1	1044	fastutil
1	1045	ints
1	1046	Int2ObjectMap
1	1047	Int2ObjectOpenHashMap
1	1048	IntArrayList
1	1049	IntList
1	1050	 'counts' contains in order: own count, context count (sum of successor's counts),
1	1051	 no of distinct successors seen once, twice, up to the COCcutoff in Configuration
1	1052	defaultReturnValue
1	1053	keyCode
1	1054	sort
1	1055	compareCounts
1	1056	rem
1	1057	0.9f
1	1058	int2ObjectEntrySet
1	1059	serialVersionUID
1	1060	FILES_PER_COUNTER
1	1061	TOKENS_PER_COUNTER
1	1062	simpleCounters
1	1063	graveyard
1	1064	procs
1	1065	fjp
1	1066	occupied
1	1067	task
1	1068	ptr
1	1069	baos
1	1070	k
1	1071	ex
1	1072	done
1	1073	q
1	1074	len
1	1075	freq
1	1076	key1
1	1077	key2
1	1078	ByteArrayInputStream
1	1079	ByteArrayOutputStream
1	1080	ObjectInputStream
1	1081	ObjectOutputStream
1	1082	concurrent
1	1083	ForkJoinPool
1	1084	 Class for counting very large corpora.
1	1085	 Helpful especially when needing to train once on a very large corpus with only none or 'normal-sized' updates afterwards
1	1086	 (though mixing is quite fine).<br /><br />
1	1087	 Very large corpora cause slow-downs in both training and testing for the conventional 
1	1088	 due to garbage collection and binary-search lookup.
1	1089	 The 
1	1090	 solves this in three ways:
1	1091	 <li>It counts in parallel
1	1092	 <li>It serializes batches of counted files into a single byte array at train-time (dramatically reducing gc overhead)
1	1093	 <li>Finally, when done training it resolves the serialized counters in parallel into a 
1	1094	VirtualCounter
1	1095	 and defers all future calls to that object.
1	1096	 in turn also has mechanisms to deal better with parallel updating and lookup than the 
1	1097	8266734684040886875L
1	1098	100
1	1099	byte
1	1100	Runtime
1	1101	getRuntime
1	1102	availableProcessors
1	1103	synchronizedList
1	1104	resolve
1	1105	submitTask
1	1106	 Submit the indices to be counted to the global ForkJoinPool.
1	1107	 This method live-locks the main thread if the fjp has more than 1K waiting tasks, to prevent flooding the JVM.
1	1108	 <br/>
1	1109	 For simplicity, we use the same method for counting and batch
1	1110	 counting and distinguish only inside the task submission, hence the type erasure in the signature.
1	1111	SuppressWarnings
1	1112	"unchecked"
1	1113	getNextAvailable
1	1114	getPoolSize
1	1115	submit
1	1116	testGraveYard
1	1117	merge
1	1118	"Resolving to VirtualCounter"
1	1119	pack
1	1120	16
1	1121	unPackAll
1	1122	"Resolved in "
1	1123	gc
1	1124	compareLists
1	1125	toByteArray
1	1126	parallel
1	1127	print
1	1128	"%..."
1	1129	counters
1	1130	memCC
1	1131	memSC
1	1132	memDS
1	1133	frequency
1	1134	 Wraps multiple counters (specifically 
1	1135	s) to support faster concurrent access 
1	1136	 and reduce garbage collection cost per counter.<br />
1	1137	 Each counter c_i is assigned all (integer) sequences s=[t0, t1, ...] such that (t0 % counters.size()) == i.
1	1138	 The empty sequence is assigned to c_0.
1	1139	 is the primary user of this VirtualCounter
1	1140	 since it must both deal with high memory usage (and thus GC) and since it 'unpacks' its internal counters in parallel. <br />
1	1141	 The choice of 
1	1142	 internally further boosts counting throughput (and reduces gc) at the expense of (slight) memory increase,
1	1143	 since that Trie uses hashmaps instead of binary-sorted arrays internally.<br /><br />
1	1144	 API note: as stated, the current implementation uses a hard-coded Counter type, which is known to synchronize its update method.
1	1145	 Future versions may explicitly synchronize counting themselves and allow generic counters to be stored.
1	1146	mapToInt
1	1147	getIndex
1	1148	marshallerFactory
1	1149	configuration
1	1150	is
1	1151	unmarshaller
1	1152	os
1	1153	marshaller
1	1154	org
1	1155	jboss
1	1156	marshalling
1	1157	Marshaller
1	1158	MarshallerFactory
1	1159	Marshalling
1	1160	MarshallingConfiguration
1	1161	Unmarshaller
1	1162	getProvidedMarshallerFactory
1	1163	"river"
1	1164	setVersion
1	1165	"Reading counter from: "
1	1166	createUnmarshaller
1	1167	createByteInput
1	1168	finish
1	1169	"Un-marshalling failed: "
1	1170	"Writing counter to: "
1	1171	createMarshaller
1	1172	createByteOutput
1	1173	"Marshalling failed: "
1	1174	Externalizable
1	1175	 Interface for counter implementations that can be used by count-based models,
1	1176	 most notably the 
1	1177	 which provides a rather efficient implementation
1	1178	 that is currently used by the 
1	1179	 Convenience method, returns count of Counter object (e.g. root node in trie-counter)
1	1180	of this Counter
1	1181	 Returns [context-count, count] pair of 
1	1182	 indices
1	1183	, for convenient MLE.
1	1184	 Note: poorly defined on empty list.
1	1185	Sequence of stored, translated tokens to return counts for
1	1186	The
1	1187	stored [context-count, count] pair of indices
1	1188	 Returns the number of sequences of length n seen `count' times
1	1189	modeledFiles
1	1190	statistics
1	1191	"Modeled %d tokens, average entropy:\t%.4f\n"
1	1192	sentenceMarkers
1	1193	regex
1	1194	lexLinesSeparately
1	1195	useDelimiters
1	1196	directory
1	1197	fIn
1	1198	from
1	1199	to
1	1200	path
1	1201	fOut
1	1202	 This class can be used to run a 
1	1203	 over bodies of code.
1	1204	 It differentiates between lexing each line separately or each file as a whole,
1	1205	 and adds several options, such as adding markers at the start and end of every sentence,
1	1206	 and only lexing files that match some extension/regular expression.
1	1207	 It also provides some util methods like 
1	1208	 and variants.
1	1209	".*"
1	1210	 Create a LexerRunner that wraps a 
1	1211	 and adds line separation if needed.
1	1212	 In some tasks (especially in NLP), a file with unrelated individual sentences on each line tends to be used,
1	1213	 whereas in most code applications, we tend to use a complete code file in which the lines should be treated as a continuous block.
1	1214	 The LexerRunner (and ModelRunner, which uses this class) need to know this to allow appropriate training.
1	1215	A 
1	1216	 that can produce a stream of tokens for each line in a File, or for single-line inputs.
1	1217	Whether the data that this LexerRunner will consider is logically grouped by lines or files.
1	1218	 Returns the lexer currently used by this class
1	1219	 Returns whether lexing adds delimiters per line.
1	1220	 Convenience method that adds sentence markers if those aren't yet present in the data.
1	1221	 A 
1	1222	 always uses the first token as a ground truth (and thus does not model it)
1	1223	 and models up to and including the last token.
1	1224	 If set to 'true', this adds delimiters (i.e. "&lt;s&gt;" and "&lt;/s&gt;"; see 
1	1225	) to each sentence.
1	1226	 A sentence is either every line in a file (if this LexerRunner is created to lex lines separately) or a whole file.
1	1227	Whether to add delimiters to each sentence. Default: false, which assumes these have already been added.
1	1228	 Returns whether or not file/line (depending on 
1	1229	 perLine
1	1230	) sentence markers are added.
1	1231	hasSentenceMarkers
1	1232	 Specify regex for file extensions to be kept.
1	1233	 <em>Note:</em> to just specify the extension, use the more convenient 
1	1234	Regular expression to match file name against. E.g. ".*\\.(c|h)" for C source and header files.
1	1235	 Alternative to 
1	1236	 that allows you to specify just the extension.
1	1237	 <em>Note:</em> this prepends <code>.*\\.</code> to the provided regex!
1	1238	Regular expression to match against extension of files. E.g. "(c|h)" for C source and header files.
1	1239	".*\\."
1	1240	 Returns the regex currently used to filter input files to lex.
1	1241	getRegex
1	1242	 Returns whether the file matches the regex and will thus be lexed by this class
1	1243	getName
1	1244	 Lex each file in this directory to a their tokens grouped by lines, subject to the underlying 
1	1245	 and whether this Lexer is configured to work per line
1	1246	 Lex the provided file to a stream of tokens per line. Note that this is preferred over lex(lines),
1	1247	 since knowing the file location/context can be helpful for most lexers!
1	1248	 <em>Note:</em> returns empty stream if the file does not match this builder's regex
1	1249	 (which accepts everything unless set otherwise in 
1	1250	File to lex
1	1251	empty
1	1252	lexTokens
1	1253	 Lex the provided text to a stream of tokens per line.
1	1254	 <b>Note:</b> if possible, use lex(File) instead! Knowing the file location/context can benefit e.g. AST lexers.
1	1255	Textual content to lex
1	1256	lexLine
1	1257	concat
1	1258	 Lex a directory recursively, provided for convenience.
1	1259	 Creates a mirror-structure in 'to' that has the lexed (and translated if 
1	1260	preTranslate
1	1261	 is set) file for each input file
1	1262	Source file/directory to be lexed
1	1263	Target file/directory to be created with lexed (optionally translated) content from source
1	1264	The Vocabulary to translate the words to indices in said Vocabulary.
1	1265	 					 If no translation is required, use 
1	1266	"Lexing at file "
1	1267	substring
1	1268	mkdirs
1	1269	"Exception in LexerBuilder.tokenize(), from "
1	1270	" to "
1	1271	withDelimiters
1	1272	function
1	1273	Function
1	1274	"\\s+"
1	1275	splitCarefully
1	1276	 Splits on punctuation but leaves start/end-of-line and unk delimiters intact
1	1277	"<((/)?s|unk)>"
1	1278	"((?<=\\p{Punct})|(?=\\p{Punct}))"
1	1279	text
1	1280	SkipEmptyLexer
1	1281	skipEmpty
1	1282	LINE_CUTOFF
1	1283	validDir
1	1284	trainOut
1	1285	testOut
1	1286	validOut
1	1287	dir
1	1288	Util
1	1289	 Including for legacy reasons only!
1	1290	IntsCreator
1	1291	"Train"
1	1292	"Test"
1	1293	"Valid"
1	1294	"ix-train"
1	1295	"ix-test"
1	1296	"ix-valid"
1	1297	"vocab.out"
1	1298	writeIXs
1	1299	getFiles
1	1300	1.1
1	1301	" "
1	1302	reversed
1	1303	ReverseLexer
1	1304	AddEndMarkers
1	1305	addEOL
1	1306	scanner
1	1307	lineTokens
1	1308	nextToken
1	1309	ln
1	1310	val
1	1311	body
1	1312	ID_REGEX
1	1313	HEX_REGEX
1	1314	BIN_REGEX
1	1315	IR_REGEX
1	1316	DBL_REGEXA
1	1317	DBL_REGEXB
1	1318	DBL_REGEXC
1	1319	DBL_REGEXD
1	1320	KEYWORDS
1	1321	KEYWORD_SET
1	1322	eclipse
1	1323	jdt
1	1324	ToolFactory
1	1325	compiler
1	1326	IScanner
1	1327	ITerminalSymbols
1	1328	InvalidInputException
1	1329	tokenizeLines
1	1330	createScanner
1	1331	"1.8"
1	1332	setSource
1	1333	toCharArray
1	1334	getNextToken
1	1335	getLineNumber
1	1336	getCurrentTokenStartPosition
1	1337	TokenNameEOF
1	1338	getCurrentTokenSource
1	1339	"\""
1	1340	endsWith
1	1341	15
1	1342	"\"\""
1	1343	"\\\\"
1	1344	"\\\\\\\\"
1	1345	"\\\\\""
1	1346	"\r"
1	1347	"\\r"
1	1348	"\'"
1	1349	">>+"
1	1350	"[,\\.\\?\\[\\]]"
1	1351	Character
1	1352	isUpperCase
1	1353	charAt
1	1354	"extends"
1	1355	"super"
1	1356	"(byte|short|int|long|float|double)"
1	1357	"(<|>)+"
1	1358	">"
1	1359	"[a-zA-Z_$][a-zA-Z\\d_$]*"
1	1360	"0x([0-9a-fA-F]+_)*[0-9a-fA-F]+[lLfFdD]?"
1	1361	"0b([01]+_)*[01]+[lL]"
1	1362	"([0-9]+_)*[0-9]+[lLfFdD]?"
1	1363	"[0-9]+\\.[0-9]+([eE][-+]?[0-9]+)?[fFdD]?"
1	1364	"[0-9]+\\.([eE][-+]?[0-9]+)?[fFdD]?"
1	1365	"\\.[0-9]+([eE][-+]?[0-9]+)?[fFdD]?"
1	1366	"[0-9]+[eE][-+]?[0-9]+[fFdD]?"
1	1367	isID
1	1368	isKeyword
1	1369	isNR
1	1370	"("
1	1371	"|"
1	1372	")"
1	1373	isSTR
1	1374	"\".+\""
1	1375	isChar
1	1376	"'.+'"
1	1377	contains
1	1378	"abstract"
1	1379	"assert"
1	1380	"boolean"
1	1381	"break"
1	1382	"byte"
1	1383	"case"
1	1384	"catch"
1	1385	"class"
1	1386	"const"
1	1387	"continue"
1	1388	"default"
1	1389	"do"
1	1390	"double"
1	1391	"else"
1	1392	"enum"
1	1393	"final"
1	1394	"finally"
1	1395	"float"
1	1396	"for"
1	1397	"goto"
1	1398	"if"
1	1399	"implements"
1	1400	"import"
1	1401	"instanceof"
1	1402	"int"
1	1403	"interface"
1	1404	"long"
1	1405	"native"
1	1406	"new"
1	1407	"package"
1	1408	"private"
1	1409	"protected"
1	1410	"public"
1	1411	"return"
1	1412	"short"
1	1413	"static"
1	1414	"strictfp"
1	1415	"switch"
1	1416	"synchronized"
1	1417	"this"
1	1418	"throw"
1	1419	"throws"
1	1420	"transient"
1	1421	"try"
1	1422	"void"
1	1423	"volatile"
1	1424	"while"
1	1425	"true"
1	1426	"false"
1	1427	"null"
1	1428	asList
1	1429	 Lex all the lines in the provided file. Use of this method is preferred, since some Lexers benefit from knowing
1	1430	 the file path (e.g. AST Lexers can use this for type inference).
1	1431	 By default, invokes 
1	1432	 with content of file.
1	1433	The file to be lexed
1	1434	Stream of lines, where every line is lexed to a Stream of tokens
1	1435	joining
1	1436	 Lex the provided text. The default implementation invokes 
1	1437	 on each line in the text,
1	1438	 but sub-classes may opt to lex the text as a whole instead (e.g. JavaLexer needs to do so to handle comments correctly).
1	1439	The text to be lexed
1	1440	 Lex the provided line into a stream of tokens.
1	1441	 The default implementations of 
1	1442	 refer to this method,
1	1443	 but sub-classes may override that behavior to take more advantage of the full content.
1	1444	The line to be lexed
1	1445	Stream of tokens that are present on this line (may be an empty Stream).
1	1446	child
1	1447	singletonList
1	1448	v
1	1449	b
1	1450	asPair
1	1451	V
1	1452	B
1	1453	"rawtypes"
1	1454	" :: "
